[TOC]

# 近似推断



## Bayesian 推断的基本思路

贝叶斯推断基于的原则是贝叶斯公式和参数空间的遍历定理。

首先回顾贝叶斯公式
$$
P(hypo|data) = \frac{P(data|hypo)P(hypo)}{P(data)}
$$
这使得我们可以进行给定样本对假设的检验和推断，从而更好地得到我们想要的"模型"（hypothesis）。

贝叶斯推断需要一个额外的性质，即“条件概率也是概率”。我们将上式增加一个条件：数据是某模型在取某个参数的时候生成的。数学上就是在贝叶斯公式的每一个概率后面，增加一个条件：也就是模型$m$。即
$$
P(hypo|data,model) = \frac{P(data|hypo,model)P(hypo|model)}{P(data|model)}
$$
这个变化给了模型极大的容量，即当我们假设数据生成*模式*给定的时候，统计推断问题就变成了模型中的*参数估计*问题。这个框架的两个主要用途是：

1. 预测（给定模型后，数据由不同的参数 $\theta$ 产生的概率是不同的。我们通过对参数空间的积分可以得到一个比较合适的预测解）
   $$
   P(x|data,model) = \int P(x|\theta, data,model)P(\theta|data,model) d\theta
   $$

2. 模型比较(不同的模型可以近似参数的结果是不一样的。比如模型取线性函数时，无论参数如何取，都不能很好的逼近二次函数的数据模式。我们需要度量模型是否能比较合适地符合数据)
   $$
   P(model|data) = \frac{P(data|model)P(model)}{P(data)}\\
   \Rightarrow P(data|model) = \int P(data|\theta,model)P(\theta|model)d\theta
   $$

一个核心任务是计算后验分布：$p(\Theta|data) = p(Z|X)$ 或者关于这个后验分布地一个函数，比如说这个分布下的对数期望（EM算法的核心思想）。为了完成这个任务，我们有一些近似的算法，比如：

- 确定性方法：Laplace近似和变分推断。
- 随机方法：MCMC



### Laplace近似

用高斯分布来近似目标后验分布，均值选取在最大似然点$z_0$（众数）附近。最大似然点的导数是$0$。所以我们对对数分布$f(z)$进行二次展开得到
$$
\ln f(z)\simeq \ln f(z_0) - \frac{1}{2}A (z-z_0)^2,\\ 
\text{其中} A = - \frac{d^2}{dz^2}\ln f(z) |_{z_0}
$$
两边同时取指数，我们就能求得$f(z)\simeq \exp{-\frac{A}{2}(z-z_0)^2}$这个形式正比于一个正态函数$N(z_0,\frac{1}{A})$。配一下系数就是我们要的根据数据得到的后验概率$q(z) = (\frac{A}{2\pi})^{1/2}\exp\{-\frac{A}{2}(z-z_0)^2\}$. 

计算的时候，先用样本众数替代$z_0$，然后算样本的二阶中心距$E(data-z_0)^2$作为$A$的估计即可。

**多维情形**是类似的。只需要将原先的（负的）二阶导数$A$换成（负的）Hessian矩阵即可。这个Hessian矩阵的意义是正态分布的协方差矩阵的逆矩阵（原因是：$M$个独立正态变量的线性组合得到了$m$维向量$z$，从而$z$的协方差矩阵是变换的逆矩阵）。
$$
q(z) = (\frac{|A|^{1/2}}{(2\pi)^{M/2}})^{1/2}\exp\{-\frac{1}{2}(z-z_0)^tA(z-z_0)\} = N(z-z_0, A^{-1})
$$
这个近似比较适合各个样本独立生成的情形，即单模式/单峰的数据。这是由于中心极限定理的存在：独立同分布的随机变量的算术平均值随着样本量的增加，会趋近于正态分布。所以进一步我们可以得知：样本量越大，拉普拉斯近似对单模式数据的近似效果越好。

### 变分推断

用一族的参数的分布$q(Z|\omega)$来近似后验概率$p(Z|data)$。这是一个优化问题：选取最好的概率分布族。

怎么评价好呢？自然是近似效果好了。所以我们要引入两个分布之间差距的度量方式：KL散度。
$$
\ln p(X) = L(q) + KL(q\| p)\\
L(q) = \int q(Z) \ln\{\frac{p(X,Z)}{q(Z)}\}dZ\\
KL(q\| p) = -\int q(Z) \ln\{\frac{p(Z|X)}{q(Z)}\}dZ
$$

- KL散度是非负的。证明如下：

  $\ln(·)$函数是凸函数：先平均（即期望）再函数 $\ge$ 先函数再期望：
  $$
  \ln E(y) \geq E(\ln(y)) 
  $$
  所以
  $$
  KL(q\|p) = -\int q(Z) \ln\frac{p(Z|X)}{q(Z)}dZ\\
  = -E_{q(Z)} \ln \frac{p(Z|X)}{q(Z)}\\
  \geq -\ln E_{q(Z)}\frac{p(Z|X)}{q(Z)}\\
  = -\ln \int\frac{p(Z|X)}{q(Z)}q(Z)dZ\\
  = -\ln \int p(Z|X)dz \text{(对概率积分为 )1}\\
  =-\ln 1 = 0
  $$

- 这般如此，剩余的$L(q)$就是分布估计的一个下界，成为Evidence lower bound（ELBO）。我们估计$L(q)$就可以了。

**变分**和处理数的函数的普通微积分相对。它最终寻求的是极值函数：它们使得泛函取得极大或极小值。在我们这里，$q$对应着一个分布，优化$q$是我们的目标，所以这个部分称为“变分推断”。

求解变分推断的方法基于一个假设：分布族的不同参数之间是独立的。这就是通常所说的“平均场假设”。

这个独立性使得密度函数可以分解为单个参数的分布的乘积：
$$
q(Z) = \prod_{i=1}^{M}q_i (Z_i)
$$
其中$M$是参数的个数。有了独立性和定义域的凸性，我们在优化ELBO的时候可以对单个参数进行优化。方法是将其余的变量的积分结果认为是常数，不去管它。我们有
$$
L(q) = \int \prod_i q_i\{\ln p(X,Z) - \sum_i \ln q_i \}dZ\\
= \int q_jdZ_j\cdot \{\int \ln p(X,Z)\prod_{i\ne j}q_i dZ_i \} - \int q_j \ln q_j dZ_j +C\\
:= \int q_j \ln \tilde{p}(X,Z_j) dZ_j - \int q_j\ln q_j dZ_j +C\\
$$
其中$\ln \tilde{p}(X,Z_j) = \mathbb{E}_{i\ne j}[\ln p(X,Z)]+C$是除去$j$之外部分的积分，即上式第二行大括号括起来的部分。

既然我们可以对单个变量进行优化，依次对所有变量进行优化即可得到想要的结果。我们让单个变量的对数似然函数$\ln q_j(Z_j)$的最优解$\ln q_j^*(Z_j)$等于样本分布函数$p$对其他参数的积分结果$\ln \tilde{p}(X,Z_j)$，我们可以得到每个分量$j$（在其他参量固定的条件下）的最优分布（还要依赖一点独立性假设）
$$
q_j^*(Z_j) = \frac{\exp(\mathbb{E}_{i\ne j}[\ln p(X,Z)])}{\int \mathbb{E}_{i\ne j}[\ln p(X,Z)] dZ_j}
$$
分母是对参数的所有可能情况遍历，即归一化。可以逐个对参数$Z_j$进行优化得到想要的结果。一定会收敛到局部最小值。特别的，如果有凸性存在，可以得到全局最小值。

变分法（特别是存在平均场假设时）对于[指数族分布](https://www.cnblogs.com/huangshiyu13/p/6820729.html)的样本特别管用。因为指数分布的无记忆性和独立性存在着天然的对应。指数族分布的一般形式是
$$
p(x) = h(x) e^{\theta^T T(x)-A(\theta)}
$$
其中$A(\theta)$的意义是归一化常数，$T$则是每次采样的权重，代表着$x$的充分统计量。比如正态分布的充分统计量就是均值方差；二项分布就是$(n,p)$等等，包含样本全部信息的参数集合，就是充分统计量。

 回到变分推断的主题：固定其他参数可以相当于其他参数确定取值是“条件”，即我们研究的是$p(z_j|z_i,i\ne j, data):=p(z_j|z_{-j}, data)$。带入指数族分布的公式，有$h(z_j)\exp\{\eta(z_{-j},data)^T T(data) \}$. 我们来试试找找指数族分布的分布估计：
$$
\log p(z_j|z_i,i\ne j, data)=\log h(z_j) +\eta(z_{-j},x)^T T(z_j) - a(\eta(z_{-j},x))\\
\text{取期望: } E\text{左}=\log h(z_j) +E[\eta(z_{-j},x)]^T T(z_j)- Ea(\eta(z_{-j},x))
$$
注意到最后一项与$q_j$无关。令左边的期望等于$\ln q^*(z_j)$的期望，可以得到$q$正比于前两项的指数：$q*(z_j)\propto h(z_j)\exp{E[\eta(z_{-j},x)]^TT(z_j)}$ 。进而我们也可以得知之后一项是标准化常数，即12式的分母。这个关系可以进一步用来构造MCMC。

- 猜测第$m$个参数可以用上前面所有$m-1$个分布作为条件。*作为条件*在概率的意义上是相乘。即
  $$
  q(z_{1:m}|\nu) = \prod_{i=1}^m q(z_j|\nu_j)
  $$

- 逐个优化算法可以得到每个分布$\nu_j$的最优取值$\nu_j^* = E[\eta(z_{-j},data)]$。



### 高斯混合模型

我们之前计算的“分布”，可以看作是均匀分布的函数，也即$z\sim U[0,1]$。经过分布$q_j$的变换，得到了一个复杂的分布模式，再变换，得到整个数据的多参数近似解$q(\vec{\mathbb{z}})$。

这类问题我们称作“分布估计”。均值方差等等的估计我们可以统称为“点估计”。分布估计比点估计难得多。